<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5"><title>pytorch基础 | 哈皮🐖</title><meta name="description" content="pytorch基础"><meta name="keywords" content="pytorch,入门"><meta name="author" content="Paul Yu"><meta name="copyright" content="Paul Yu"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="preconnect" href="https://fonts.googleapis.com" crossorigin><link rel="preconnect" href="//busuanzi.ibruce.info"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="pytorch基础"><meta name="twitter:description" content="pytorch基础"><meta name="twitter:image" content="http://image-paul-blogs.test.upcdn.net/blog/a13.jpg"><meta property="og:type" content="article"><meta property="og:title" content="pytorch基础"><meta property="og:url" content="http://yoursite.com/2020/03/08/pytorch%E5%9F%BA%E7%A1%80/"><meta property="og:site_name" content="哈皮🐖"><meta property="og:description" content="pytorch基础"><meta property="og:image" content="http://image-paul-blogs.test.upcdn.net/blog/a13.jpg"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://cdn.jsdelivr.net/npm/js-cookie/dist/js.cookie.min.js"></script><script>const autoChangeMode = 'false'
var t = Cookies.get("theme");
if (autoChangeMode == '1'){
const isDarkMode = window.matchMedia("(prefers-color-scheme: dark)").matches
const isLightMode = window.matchMedia("(prefers-color-scheme: light)").matches
const isNotSpecified = window.matchMedia("(prefers-color-scheme: no-preference)").matches
const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

if (t === undefined){
  if (isLightMode) activateLightMode()
  else if (isDarkMode) activateDarkMode()
  else if (isNotSpecified || hasNoSupport){
    console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
    now = new Date();
    hour = now.getHours();
    isNight = hour < 6 || hour >= 18
    isNight ? activateDarkMode() : activateLightMode()
}
} else if (t == 'light') activateLightMode()
else activateDarkMode()


} else if (autoChangeMode == '2'){
  now = new Date();
  hour = now.getHours();
  isNight = hour < 6 || hour >= 18
  if(t === undefined) isNight? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode() 
} else {
  if ( t == 'dark' ) activateDarkMode()
  else if ( t == 'light') activateLightMode()
}

function activateDarkMode(){
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null){
    document.querySelector('meta[name="theme-color"]').setAttribute('content','#000')
  }
}
function activateLightMode(){
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null){
  document.querySelector('meta[name="theme-color"]').setAttribute('content','#fff')
  }
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="canonical" href="http://yoursite.com/2020/03/08/pytorch%E5%9F%BA%E7%A1%80/"><link rel="prev" title="可视化之Matplotlib(一)" href="http://yoursite.com/2020/03/12/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8BMatplotlib(%E4%B8%80)/"><link rel="next" title="神经网络--线性模型与梯度下降" href="http://yoursite.com/2020/03/08/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://xxx/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: 'Snackbar.bookmark.title',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: true,
  copyright: undefined,
  ClickShowText: {"text":"富强,民主,文明,和谐,自由,平等,公正,法治,爱国,敬业,诚信,友善","fontSize":"15px"},
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  baiduPush: false,
  isHome: false,
  isPost: true
  
}</script><meta name="generator" content="Hexo 4.2.0"></head><body><header> <div id="page-header"><span class="pull_left" id="blog_name"><a class="blog_title" id="site-name" href="/">哈皮🐖</a></span><span class="toggle-menu pull_right close"><a class="site-page"><i class="fa fa-bars fa-fw" aria-hidden="true"></i></a></span><span class="pull_right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/download/"><i class="fa-fw fa fa-download"></i><span> 下载站</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 娱乐</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-picture-o"></i><span> 相册</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></span></div></header><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">11</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">11</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">13</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/download/"><i class="fa-fw fa fa-download"></i><span> 下载站</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fa fa-list" aria-hidden="true"></i><span> 娱乐</span><i class="fa fa-chevron-down menus-expand" aria-hidden="true"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/gallery/"><i class="fa-fw fa fa-picture-o"></i><span> 相册</span></a></li><li><a class="site-page" href="/movies/"><i class="fa-fw fa fa-film"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></div><div id="mobile-sidebar-toc"><div class="toc_mobile_headline">目录</div><div class="sidebar-toc__content"><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#pytorch基础"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text"> pytorch基础</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#张量tensor"><span class="toc_mobile_items-number">1.1.</span> <span class="toc_mobile_items-text"> 张量(Tensor)</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-1"><a class="toc_mobile_items-link" href="#pytorch自动微分"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text"> Pytorch自动微分</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#tensor"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text"> TENSOR</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#自动求导"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text"> 自动求导</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#简单情况下的自动求导"><span class="toc_mobile_items-number">2.2.1.</span> <span class="toc_mobile_items-text"> 简单情况下的自动求导</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#复杂情况下的自动求导"><span class="toc_mobile_items-number">2.2.2.</span> <span class="toc_mobile_items-text"> 复杂情况下的自动求导</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#多次自动求导"><span class="toc_mobile_items-number">2.2.3.</span> <span class="toc_mobile_items-text"> 多次自动求导</span></a></li></ol></li></ol></li></ol></div></div></div><div id="body-wrap"><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true">     </i><div class="auto_open" id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch基础"><span class="toc-number">1.</span> <span class="toc-text"> pytorch基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#张量tensor"><span class="toc-number">1.1.</span> <span class="toc-text"> 张量(Tensor)</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch自动微分"><span class="toc-number">2.</span> <span class="toc-text"> Pytorch自动微分</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#tensor"><span class="toc-number">2.1.</span> <span class="toc-text"> TENSOR</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#自动求导"><span class="toc-number">2.2.</span> <span class="toc-text"> 自动求导</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#简单情况下的自动求导"><span class="toc-number">2.2.1.</span> <span class="toc-text"> 简单情况下的自动求导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#复杂情况下的自动求导"><span class="toc-number">2.2.2.</span> <span class="toc-text"> 复杂情况下的自动求导</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#多次自动求导"><span class="toc-number">2.2.3.</span> <span class="toc-text"> 多次自动求导</span></a></li></ol></li></ol></li></ol></div></div></div><main id="content-outer"><div id="top-container" style="background-image: url(http://image-paul-blogs.test.upcdn.net/blog/a13.jpg)"><div id="post-info"><div id="post-title"><div class="posttitle">pytorch基础</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 发表于 2020-03-08<span class="post-meta__separator">|</span><i class="fa fa-history fa-fw" aria-hidden="true"></i> 更新于 2020-03-08</time><span class="post-meta__separator">|</span><span><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><i class="fa fa-angle-right fa-fw" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon fa-fw" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/pytorch%E5%9F%BA%E7%A1%80/">pytorch基础</a></span><div class="post-meta-wordcount"><i class="fa fa-file-word-o post-meta__icon fa-fw" aria-hidden="true"></i><span>字数总计:</span><span class="word-count">1.7k</span><span class="post-meta__separator">|</span><i class="fa fa-clock-o post-meta__icon fa-fw" aria-hidden="true"></i><span>阅读时长: 7 分钟</span><div class="post-meta-pv-cv"><span class="post-meta__separator">|</span><span><i class="fa fa-eye post-meta__icon fa-fw" aria-hidden="true"> </i>阅读量:</span><span id="busuanzi_value_page_pv"></span></div></div></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><script src="/assets/js/APlayer.min.js"> </script><h1 id="pytorch基础"><a class="markdownIt-Anchor" href="#pytorch基础"></a> pytorch基础</h1>
<h2 id="张量tensor"><a class="markdownIt-Anchor" href="#张量tensor"></a> 张量(Tensor)</h2>
<p>pytorch中处理的基本对象就是Tensor，表示一个多维的矩阵。一维就是向量，二维就是一般矩阵，多维就是张量。不同数据类型的Tensor，有32位浮点型torch.FloatTensor、64位浮点型torch.DoubleTensor、16位整型torch.ShortTensor、32位整型torch.IntTensor、64位整型torch.LongTensor。torch.Tensor模式是32位浮点型。</p>
<p><a href="http://image-paul-blogs.test.upcdn.net/pytorch/p02.png" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="http://image-paul-blogs.test.upcdn.net/pytorch/p02.png" class="lazyload"></a></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.Tensor([[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">8</span>],[<span class="number">7</span>,<span class="number">9</span>]])</span><br><span class="line">print(<span class="string">'a size is &#123;&#125;'</span>.format(a.size())) <span class="comment"># 3,2</span></span><br><span class="line"><span class="comment">#创建一个全是0的空Tensor，或者取随机正态分布作为初始值</span></span><br><span class="line">b = torch.zeros((<span class="number">3</span>,<span class="number">2</span>))</span><br><span class="line">print(<span class="string">'zero tensor:&#123;&#125;'</span>.format(c))</span><br><span class="line">d = torch.randn((<span class="number">3</span>,<span class="number">2</span>))</span><br><span class="line">print(<span class="string">'normal randn is:&#123;&#125;'</span>.format(d))</span><br></pre></td></tr></table></figure></div>
<p>可以通过索引方式取得其中元素，同时也可以改变它的值。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a[<span class="number">0</span>,<span class="number">1</span>] = <span class="number">100</span></span><br></pre></td></tr></table></figure></div>
<p>创建一个tensor基于已经存在的tensor</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建一个tensor基于已经存在的tensor</span></span><br><span class="line">x = torch.tensor([<span class="number">5.5</span>,<span class="number">3</span>])</span><br><span class="line">x = x.new_ones(<span class="number">5</span>,<span class="number">3</span>,dtype=torch.double)</span><br><span class="line">print(x)</span><br><span class="line">x = torch.randn_like(x,dtype=torch.float)</span><br><span class="line">print(x)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[ 1.,  1.,  1.],</span></span><br><span class="line"><span class="string">        [ 1.,  1.,  1.],</span></span><br><span class="line"><span class="string">        [ 1.,  1.,  1.],</span></span><br><span class="line"><span class="string">        [ 1.,  1.,  1.],</span></span><br><span class="line"><span class="string">        [ 1.,  1.,  1.]], dtype=torch.float64)</span></span><br><span class="line"><span class="string">tensor([[-0.2183,  0.4477, -0.4053],</span></span><br><span class="line"><span class="string">        [ 1.7353, -0.0048,  1.2177],</span></span><br><span class="line"><span class="string">        [-1.1111,  1.0878,  0.9722],</span></span><br><span class="line"><span class="string">        [-0.7771, -0.2174,  0.0412],</span></span><br><span class="line"><span class="string">        [-2.1750,  1.3609, -0.3322]])'''</span></span><br></pre></td></tr></table></figure></div>
<p>对tensor的操作（以加法为例）：</p>
<ol>
<li>
<pre class="highlight"><code class="python">y = torch.rand(<span class="number">5</span>,<span class="number">3</span>)
print(x+y)
<span class="comment">#output</span>
<span class="string">'''
tensor([[-0.1859,  1.3970,  0.5236],
        [ 2.3854,  0.0707,  2.1970],
        [-0.3587,  1.2359,  1.8951],
        [-0.1189, -0.1376,  0.4647],
        [-1.8968,  2.0164,  0.1092]])'''</span>
&lt;!--￼<span class="number">3</span>--&gt;

</code></pre>
</li>
<li>
<pre class="highlight"><code class="python"><span class="comment">#提供一个输出tensor作为参数</span>
result = torch.empty(<span class="number">5</span>, <span class="number">3</span>)
torch.add(x, y, out=result)
print(result)
&lt;!--￼<span class="number">4</span>--&gt;

</code></pre>
</li>
</ol>
<p>还可以在Tensor和numpy.ndarray之间相互转换</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">numpy_b  = b.numpy()</span><br><span class="line">print(type(numpy_b))</span><br><span class="line">print(<span class="string">'cover to numpy is\n:&#123;&#125;'</span>.format(numpy_b))</span><br><span class="line"></span><br><span class="line">e = np.array([[<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>]])</span><br><span class="line">torch_e = torch.from_numpy(e)</span><br><span class="line">print(<span class="string">'from numpy to torch.Tensor is &#123;&#125;:'</span>.format(torch_e))</span><br><span class="line">f_torch_e = torch_e.float()</span><br><span class="line">print(<span class="string">'changed data type to float tensor:&#123;&#125;'</span>.format(f_torch_e))</span><br></pre></td></tr></table></figure></div>
<p>通过numpy()函数就能将tensor类型转为numpy类型。同时使用torch.from_numpy()就能将numpy转换为tensor类型。</p>
<p>如果要改变一个tensor的大小和形状，可以通过torch.view</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">16</span>)</span><br><span class="line">z = x.view(<span class="number">-1</span>, <span class="number">8</span>)  <span class="comment"># the size -1 is inferred from other dimensions</span></span><br><span class="line">print(x.size(), y.size(), z.size())</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])'''</span></span><br></pre></td></tr></table></figure></div>
<p>如果有一个元素tensor可以使用.item获取tensor的值</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>)</span><br><span class="line">print(x)</span><br><span class="line">print(x.item())</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([ 0.9422])</span></span><br><span class="line"><span class="string">0.9422121644020081'''</span></span><br></pre></td></tr></table></figure></div>
<h1 id="pytorch自动微分"><a class="markdownIt-Anchor" href="#pytorch自动微分"></a> Pytorch自动微分</h1>
<h2 id="tensor"><a class="markdownIt-Anchor" href="#tensor"></a> TENSOR</h2>
<p><code>torch.Tensor</code>是包的核心类。如果将属性<code>.requires_grad</code>设置为<strong>True</strong>，则会开始跟踪针对tensor的所有操作。完成计算后，可以调用<code>.backward()</code>来自动计算所有梯度。该张量的梯度将累积到<code>.grad</code>属性中。</p>
<p>要停止tensor历史记录追踪，可以调用.detach()。它将其与计算历史记录分离，并防止将来的计算被追踪。</p>
<p>还有一个类对于实现autograd非常重要，就是Function。Tensor和Function互相连接并构成了一个非循环图，它保存了完整的计算过程历史信息。每个张量都有一个.grad_fn属性保存着创建了张量的Function引用。</p>
<p>如果想计算导数可以调用Tensor.backward(),如果Tensor是标量，即只有一个数据元素，则不需要指定任何参数backward()。但如果有更多元素，则需要指定一个<code>gradient</code>参数来指定张量的形状。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.ones(<span class="number">2</span>,<span class="number">2</span>,requires_grad=true)</span><br><span class="line">print(x)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1.]], requires_grad=True)'''</span></span><br></pre></td></tr></table></figure></div>
<p>针对张量x做操作</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">y = x+<span class="number">2</span></span><br><span class="line">print(y)</span><br><span class="line">print(y.grad_fn)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[3., 3.],</span></span><br><span class="line"><span class="string">        [3., 3.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">&lt;AddBackward0 object at 0x7fe1db427470&gt;'''</span></span><br></pre></td></tr></table></figure></div>
<p>针对y做更多操作：</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">z = y*y*<span class="number">3</span></span><br><span class="line">out = z.mean()</span><br><span class="line">print(z,out)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[27., 27.],</span></span><br><span class="line"><span class="string">        [27., 27.]], grad_fn=&lt;MulBackward0&gt;) </span></span><br><span class="line"><span class="string">tensor(27., grad_fn=&lt;MeanBackward0&gt;)'''</span></span><br></pre></td></tr></table></figure></div>
<p>.requires_grad(…)会改变张量的requires_grad标记。输入的标记默认是False。如果不提供相应参数。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">a = ((a * <span class="number">3</span>) / (a - <span class="number">1</span>))</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">a.requires_grad_(<span class="literal">True</span>)</span><br><span class="line">print(a.requires_grad)</span><br><span class="line">b = (a * a).sum()</span><br><span class="line">print(b.grad_fn)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">False</span></span><br><span class="line"><span class="string">True</span></span><br><span class="line"><span class="string">&lt;SumBackward0 object at 0x7fe1db427dd8&gt;'''</span></span><br></pre></td></tr></table></figure></div>
<p>现在向后传播，因为输出包含了一个标量，out.backward()等同于out.backward(torch.tensor(1.)).</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">out.backward()</span><br><span class="line"><span class="comment">#打印梯度</span></span><br><span class="line">print(x.grad)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([[4.5000, 4.5000],</span></span><br><span class="line"><span class="string">        [4.5000, 4.5000]])'''</span></span><br></pre></td></tr></table></figure></div>
<p>原理解释：</p>
<p><a href="http://image-paul-blogs.test.upcdn.net/pytorch/p01.jpeg" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="http://image-paul-blogs.test.upcdn.net/pytorch/p01.jpeg" class="lazyload"></a></p>
<blockquote>
<p>scalar funcation:标量函数</p>
<p>l = ay1+by2+…kym</p>
</blockquote>
<p>继续看一个雅克比向量积的例子</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">y = x*<span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm()&lt;<span class="number">1000</span>:</span><br><span class="line">    y=y*<span class="number">2</span></span><br><span class="line">print(y)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([ -444.6791,   762.9810, -1690.0941], grad_fn=&lt;MulBackward0&gt;)'''</span></span><br></pre></td></tr></table></figure></div>
<p>在这种情况下，y不再是一个标量。torch.autograd不能直接计算整个的雅可比，但是如果只想要雅克比向量积，只需要简单的传递向量给backward作为参数(a,b…k)，其大小等于y.size。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">v = torch.tensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>], dtype=torch.float)</span><br><span class="line">y.backward(v)</span><br><span class="line"></span><br><span class="line">print(x.grad)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])'''</span></span><br></pre></td></tr></table></figure></div>
<h2 id="自动求导"><a class="markdownIt-Anchor" href="#自动求导"></a> 自动求导</h2>
<p>自动求导是pytorch非常重要特性，能够避免手动计算非常复杂的导数。</p>
<h3 id="简单情况下的自动求导"><a class="markdownIt-Anchor" href="#简单情况下的自动求导"></a> 简单情况下的自动求导</h3>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.Tensor([<span class="number">2</span>],requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x+<span class="number">2</span></span><br><span class="line">z = y**<span class="number">2</span> +<span class="number">3</span></span><br><span class="line">print(z)</span><br><span class="line"><span class="comment">#outputs</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 19</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 1]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure></div>
<p>通过上面操作得到了结果就是z，我们可以将其表示为数学公式：</p>
<p><code>z = (x+2)^2+3</code></p>
<p>那么从z对x求导就是</p>
<p><code>dz/dx = 2(x+2)=2(2+2)=8</code></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用自动求导</span></span><br><span class="line">z.backward()</span><br><span class="line">print(x.grad)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 8</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 1]'''</span></span><br></pre></td></tr></table></figure></div>
<h3 id="复杂情况下的自动求导"><a class="markdownIt-Anchor" href="#复杂情况下的自动求导"></a> 复杂情况下的自动求导</h3>
<p>上面是对标量求导，也可以对向量或者矩阵自动求导。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">m = torch.FloatTensor([[<span class="number">2</span>,<span class="number">3</span>]],requires_grad=<span class="literal">True</span>)<span class="comment">#构建1*2矩阵</span></span><br><span class="line">n = torch.zeros(<span class="number">1</span>,<span class="number">2</span>)<span class="comment">#构建大小相同的0矩阵</span></span><br><span class="line">print(m)</span><br><span class="line">print(n)</span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 2  3</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 1x2]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 0  0</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 1x2]'''</span></span><br></pre></td></tr></table></figure></div>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#通过m中的值计算新的n中的值</span></span><br><span class="line">n[<span class="number">0</span>,<span class="number">0</span>] = m[<span class="number">0</span>,<span class="number">0</span>]**<span class="number">2</span></span><br><span class="line">n[<span class="number">0</span>,<span class="number">1</span>] = m[<span class="number">0</span>,<span class="number">1</span>]**<span class="number">3</span></span><br></pre></td></tr></table></figure></div>
<p>将上面的式子写成数学公式可得：<code>n = (n0,n1) = (m0^2,m1^3)</code></p>
<p>下面直接对n进行反向传播，也就是对n求m的导数。</p>
<p><a href="http://image-paul-blogs.test.upcdn.net/pytorch/p03.svg" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="http://image-paul-blogs.test.upcdn.net/pytorch/p03.svg" class="lazyload"></a></p>
<p>在pytorch中调用自动求导，往往需要在backward()传入一个参数，参数形状大小和n一样大。比如是（w0,w1）,那么自动求导结果就是：</p>
<p><a href="http://image-paul-blogs.test.upcdn.net/pytorch/p04.svg" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="http://image-paul-blogs.test.upcdn.net/pytorch/p04.svg" class="lazyload"></a></p>
<p><a href="http://image-paul-blogs.test.upcdn.net/pytorch/p09.svg" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="http://image-paul-blogs.test.upcdn.net/pytorch/p09.svg" class="lazyload"></a></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">n.backward(torch.ones_like(n))<span class="comment">#(1,1)</span></span><br><span class="line">print(m_grad)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string">  4  27</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 1x2]'''</span></span><br></pre></td></tr></table></figure></div>
<p><a href="http://image-paul-blogs.test.upcdn.net/pytorch/p05.svg" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="http://image-paul-blogs.test.upcdn.net/pytorch/p05.svg" class="lazyload"></a></p>
<p><a href="http://image-paul-blogs.test.upcdn.net/pytorch/p10.svg" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="http://image-paul-blogs.test.upcdn.net/pytorch/p10.svg" class="lazyload"></a></p>
<h3 id="多次自动求导"><a class="markdownIt-Anchor" href="#多次自动求导"></a> 多次自动求导</h3>
<p>通过调用backward()可以进行一次求导，而再次调用，就会报错。是因为pytorch默认做完一次自动求导后，计算图就会被丢弃，所以两次自动求导需要进行设置。</p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.FloatTensor([<span class="number">3</span>],requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = x*<span class="number">2</span>+x**<span class="number">2</span>+<span class="number">3</span></span><br><span class="line">print(y)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 18</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 1]'''</span></span><br></pre></td></tr></table></figure></div>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">y.backward(retain_graph=<span class="literal">True</span>)</span><br><span class="line">print(x.grad)</span><br><span class="line">y.backward()<span class="comment">#再做一次求导不会保留计算图</span></span><br><span class="line">print(x.grad)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 8</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 1]</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 16</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 1]'''</span></span><br></pre></td></tr></table></figure></div>
<p>例：</p>
<p>定义：<a href="http://image-paul-blogs.test.upcdn.net/pytorch/p06.svg" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="http://image-paul-blogs.test.upcdn.net/pytorch/p06.svg" class="lazyload"></a></p>
<p>希望求：<a href="http://image-paul-blogs.test.upcdn.net/pytorch/p07.svg" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="http://image-paul-blogs.test.upcdn.net/pytorch/p07.svg" class="lazyload"></a></p>
<p>参考答案：<a href="http://image-paul-blogs.test.upcdn.net/pytorch/p08.svg" target="_blank" rel="noopener" data-fancybox="group" data-caption class="fancybox"><img alt title data-src="http://image-paul-blogs.test.upcdn.net/pytorch/p08.svg" class="lazyload"></a></p>
<div class="code-area-wrap"><div class="highlight-tools"><i class="fa fa-angle-down code-expand" aria-hidden="true"></i><div class="code_lang">python</div><div class="copy-notice"></div><i class="fa fa-clipboard" aria-hidden="true"></i></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = torch.FloatTensor([<span class="number">2</span>,<span class="number">3</span>],requires_grad=<span class="literal">True</span>)</span><br><span class="line">k = torch.zeros(<span class="number">2</span>)</span><br><span class="line">k[<span class="number">0</span>] = x[<span class="number">0</span>]**<span class="number">2</span>+<span class="number">3</span>**x[<span class="number">1</span>]</span><br><span class="line">k[<span class="number">1</span>] = x[<span class="number">1</span>]**<span class="number">2</span>+<span class="number">2</span>**x[<span class="number">0</span>]</span><br><span class="line">j = torch.zeros(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">k.backward(torch.FloatTensor([<span class="number">1</span>,<span class="number">0</span>]),retain_graph=<span class="literal">True</span>)</span><br><span class="line">j[<span class="number">0</span>] = x.grad.data</span><br><span class="line">x.grad.data.zero_()<span class="comment">#归零之前求得梯度</span></span><br><span class="line">k.backward(torch.FloatTensor([<span class="number">0</span>,<span class="number">1</span>]))</span><br><span class="line">j[<span class="number">1</span>] = x.grad.data</span><br><span class="line">print(j)</span><br><span class="line"><span class="comment">#output</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">4  3</span></span><br><span class="line"><span class="string">2  6</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">'''</span></span><br></pre></td></tr></table></figure></div>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Paul Yu</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yoursite.com/2020/03/08/pytorch%E5%9F%BA%E7%A1%80/">http://yoursite.com/2020/03/08/pytorch%E5%9F%BA%E7%A1%80/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yoursite.com">哈皮🐖</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/pytorch/">pytorch    </a><a class="post-meta__tags" href="/tags/%E5%85%A5%E9%97%A8/">入门    </a></div><div class="post_share"><div class="social-share" data-image="http://image-paul-blogs.test.upcdn.net/blog/a13.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><div class="post-reward"><a class="reward-button button--primary button--animated"> <i class="fa fa-qrcode"></i> 打赏<div class="reward-main"><ul class="reward-all"><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/wechat.png" alt="微信"><div class="post-qr-code__desc">微信</div></li><li class="reward-item"><img class="lazyload post-qr-code__img" src="/img/alipay.png" alt="支付寶"><div class="post-qr-code__desc">支付寶</div></li></ul></div></a></div><nav class="pagination_post" id="pagination"><div class="prev-post pull_left"><a href="/2020/03/12/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8BMatplotlib(%E4%B8%80)/"><img class="prev_cover lazyload" data-src="http://image-paul-blogs.test.upcdn.net/blog/a2.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>可视化之Matplotlib(一)</span></div></a></div><div class="next-post pull_right"><a href="/2020/03/08/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"><img class="next_cover lazyload" data-src="http://image-paul-blogs.test.upcdn.net/blog/a9.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>神经网络--线性模型与梯度下降</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2020/03/08/神经网络/" title="神经网络--线性模型与梯度下降"><img class="relatedPosts_cover lazyload"data-src="http://image-paul-blogs.test.upcdn.net/blog/a9.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="fa fa-calendar fa-fw" aria-hidden="true"></i> 2020-03-08</div><div class="relatedPosts_title">神经网络--线性模型与梯度下降</div></div></a></div></div><div class="clear_both"></div></div><hr><div id="post-comment"><div class="comment_headling"><i class="fa fa-comments fa-fw" aria-hidden="true"></i><span> 评论</span></div><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'b7cd8445bdb1b19631d9',
  clientSecret: '7d1af09b37d2980e12c8d08d4749917555bc2cef',
  repo: 'nicePaul521.github.io',
  owner: 'nicePaul521',
  admin: 'nicePaul521',
  id: md5(decodeURI(location.pathname)),
  language: 'zh-CN',
  updateCountCallback: commentCount
})
gitalk.render('gitalk-container')

function commentCount(n){
  document.getElementsByClassName('gitalk-comment-count')[0].innerHTML= n
}</script></div></div></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 By Paul Yu</div><div class="framework-info"><span>驱动 </span><a href="http://hexo.io" target="_blank" rel="noopener"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a href="https://github.com/jerryc127/hexo-theme-butterfly" target="_blank" rel="noopener"><span>Butterfly</span></a></div><div class="footer_custom_text">hi,welcome to my<a href="https://nicepaul.top/" target="_blank" rel="noopener"> blog</a>!</div></div></footer></div><section class="rightside" id="rightside"><div id="rightside-config-hide"><i class="fa fa-book" id="readmode" title="阅读模式"></i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" title="简繁转换" target="_self">繁</a><i class="darkmode fa fa-moon-o" id="darkmode" title="夜间模式"></i></div><div id="rightside-config-show"><div id="rightside_config" title="设置"><i class="fa fa-cog" aria-hidden="true"></i></div><a id="to_comment" href="#post-comment" title="直达评论"><i class="scroll_to_comment fa fa-comments">  </i></a><i class="fa fa-list-ul close" id="mobile-toc-button" title="目录" aria-hidden="true"></i><i class="fa fa-arrow-up" id="go-up" title="回到顶部" aria-hidden="true"></i></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/calendar.js"></script><script src="/js/languages.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@latest/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lazysizes@latest/lazysizes.min.js" async=""></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/click_heart.js"></script><script src="https://cdn.jsdelivr.net/gh/jerryc127/butterfly_cdn@2.1.0/js/ClickShowText.js"></script></body></html>